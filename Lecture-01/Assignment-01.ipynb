{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:13:44.420979Z",
     "start_time": "2019-10-04T07:13:37.699961Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import jieba\n",
    "import random\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "% %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:13:48.863092Z",
     "start_time": "2019-10-04T07:13:48.859134Z"
    }
   },
   "outputs": [],
   "source": [
    "simple_grammar = \"\"\"\n",
    "sentence => noun_phrase verb_phrase\n",
    "noun_phrase => Article Adj* noun\n",
    "Adj* => Adj | Adj Adj*\n",
    "verb_phrase => verb noun_phrase\n",
    "Article =>  一个 | 这个\n",
    "noun =>   女人 |  篮球 | 桌子 | 小猫\n",
    "verb => 看着   |  坐在 |  听着 | 看见\n",
    "Adj =>  蓝色的 | 好看的 | 小小的\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:13:50.781958Z",
     "start_time": "2019-10-04T07:13:50.777968Z"
    }
   },
   "outputs": [],
   "source": [
    "hello_rules = '''\n",
    "say_hello = names hello tail \n",
    "names = name names | name\n",
    "name = Jhon | Mike | 老梁 | 老刘 \n",
    "hello = 你好 | 您来啦 | 快请进\n",
    "tail = 呀 | ！\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:15:08.650595Z",
     "start_time": "2019-10-04T07:15:08.638628Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mike您来啦呀'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate(grammar_rules, target):\n",
    "    '''从语法树种查找匹配的回答'''\n",
    "    if target in grammar_rules:\n",
    "        candidates = grammar_rules[target]\n",
    "        candidate = random.choice(candidates)\n",
    "        return ''.join(generate(grammar_rules, target=c.strip()) for c in candidate.split())\n",
    "    else:\n",
    "        return target\n",
    "    \n",
    "def get_generation_by_gram(grammar_str:str, target, stmt_split='=', or_split='|'):\n",
    "    '''grammar_str:语法树\n",
    "       target：\n",
    "       stmt_split：分隔符\n",
    "       or_split：分隔符 \n",
    "    '''\n",
    "    rules = dict()\n",
    "    for line in grammar_str.split('\\n'):\n",
    "        if not line:\n",
    "            continue\n",
    "        stmt, expr = line.split(stmt_split)\n",
    "        rules[stmt.strip()] = expr.split(or_split)\n",
    "        \n",
    "    generated = generate(rules, target=target)\n",
    "    \n",
    "    return generated\n",
    "            \n",
    "# rules = get_generation_by_gram(hello_rules, 'say_hello', stmt_split='=', or_split='|')      \n",
    "# generate(rules, 'say_hello')   \n",
    "get_generation_by_gram(hello_rules, 'say_hello', stmt_split='=', or_split='|')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:20:44.630585Z",
     "start_time": "2019-10-04T07:20:44.625597Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'这个蓝色的小猫看着这个好看的小猫'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_generation_by_gram(simple_grammar, 'sentence', stmt_split='=>', or_split='|')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:21:08.330168Z",
     "start_time": "2019-10-04T07:21:08.321191Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if(1==0b2d){10d=303}\n",
      "if(1<=0d){d=3}\n",
      "if(0b==d0){3db=31}\n",
      "if(03ddc==2){031=d2}\n",
      "if(add<dcd1){if(0<=a3){ac31=d2}}\n",
      "if(a<=13332b){if(2==33a){if(2a2==d){if(b>=1){33=0a}}}}\n",
      "if(3d0){if(d3==cd){if(1c==cd){if(00<=1d){3d=bbbb}}}}\n",
      "if(21a<1cb){baad=b0}\n",
      "if(b>=0){1=ba}\n",
      "if(3>=cd){b0=a}\n",
      "if(b<=d){0a=31}\n",
      "if(3<d){if(d==2){2=33}}\n",
      "if(ab>=b){c=b}\n",
      "if(1<=3c){1=dc}\n",
      "if(3<=0acaa){323=ab}\n",
      "if(3>=1b){if(2<0){a=b}}\n",
      "if(d32<=ba){if(a3d3<2){d=c}}\n",
      "if(cc0<3){2b=31}\n",
      "if(23==d){if(d202==0){d=c}}\n",
      "if(ada0){if(bad<=c2){bb=d}}\n"
     ]
    }
   ],
   "source": [
    "# auto coding\n",
    "simple_programming = '''\n",
    "if_stmt => if ( cond ) { stmt }\n",
    "cond => var op var\n",
    "op => | == | < | >= | <= \n",
    "stmt => assign | if_stmt\n",
    "assign => var = var\n",
    "var =>  char var | char\n",
    "char => a | b |  c | d | 0 | 1 | 2 | 3\n",
    "'''\n",
    "for i in range(20):\n",
    "    print(get_generation_by_gram(simple_programming, 'if_stmt', stmt_split='=>', or_split='|')  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:21:23.299116Z",
     "start_time": "2019-10-04T07:21:23.292170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 老梁\n",
      " 老梁\n",
      "Jhon \n",
      " 老梁\n",
      " 老梁\n"
     ]
    }
   ],
   "source": [
    "def name():\n",
    "    return random.choice('Jhon | Mike | 老梁'.split('|'))\n",
    "for i in range(5):\n",
    "    print(name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:21:31.155097Z",
     "start_time": "2019-10-04T07:21:31.149110Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 您来啦 \n",
      " 快请进\n",
      " 快请进\n",
      " 快请进\n",
      "你好 \n"
     ]
    }
   ],
   "source": [
    "def hello():\n",
    "    return random.choice('你好 | 您来啦 | 快请进'.split('|'))\n",
    "for i in range(5):\n",
    "    print(hello())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:21:38.781687Z",
     "start_time": "2019-10-04T07:21:38.775704Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mike   您来啦 \n",
      " Mike  你好 \n",
      "Jhon   快请进\n",
      " 老梁  快请进\n",
      " 老梁 你好 \n"
     ]
    }
   ],
   "source": [
    "def say_hello():\n",
    "    return name()+ ' ' + hello()\n",
    "for i in range(5):\n",
    "    print(say_hello())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:21:55.449087Z",
     "start_time": "2019-10-04T07:21:55.446097Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = \"./article_9k.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:21:57.759907Z",
     "start_time": "2019-10-04T07:21:56.569091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "contents = open(corpus,encoding='utf-8').read()\n",
    "type(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:21:59.630897Z",
     "start_time": "2019-10-04T07:21:59.623916Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33425826"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:22:02.603945Z",
     "start_time": "2019-10-04T07:22:02.596968Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'此外自本周6月12日'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T11:12:25.110634Z",
     "start_time": "2019-10-01T11:12:16.290326Z"
    }
   },
   "outputs": [],
   "source": [
    "# import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:22:16.493778Z",
     "start_time": "2019-10-04T07:22:16.487825Z"
    }
   },
   "outputs": [],
   "source": [
    "max_length = 1000000 \n",
    "sub_file = contents[:max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:25:30.042878Z",
     "start_time": "2019-10-04T07:22:23.672571Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\acer9527\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.269 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 用jieba 分词\n",
    "TOKENS = jieba.lcut(contents)\n",
    "type(TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:26:30.752433Z",
     "start_time": "2019-10-04T07:26:30.747446Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['此外', '自', '本周', '6', '月', '12', '日起', '除', '小米', '手机']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKENS[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:42:41.712346Z",
     "start_time": "2019-10-04T07:42:37.958390Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# 统计1元词频\n",
    "words_count = Counter(TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:42:47.585661Z",
     "start_time": "2019-10-04T07:42:47.447001Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('的', 703716),\n",
       " ('n', 382020),\n",
       " ('在', 263597),\n",
       " ('月', 189330),\n",
       " ('日', 166300),\n",
       " ('新华社', 142462),\n",
       " ('和', 134061),\n",
       " ('年', 123106),\n",
       " ('了', 121938),\n",
       " ('是', 100909)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_count.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:43:01.786669Z",
     "start_time": "2019-10-04T07:43:01.531314Z"
    }
   },
   "outputs": [],
   "source": [
    "words_with_freqences = [f for w,f in words_count.most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:43:07.417565Z",
     "start_time": "2019-10-04T07:43:07.410585Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[703716,\n",
       " 382020,\n",
       " 263597,\n",
       " 189330,\n",
       " 166300,\n",
       " 142462,\n",
       " 134061,\n",
       " 123106,\n",
       " 121938,\n",
       " 100909]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_with_freqences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:43:16.009572Z",
     "start_time": "2019-10-04T07:43:14.734982Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e682d585c0>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.log(words_with_freqences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:43:22.724605Z",
     "start_time": "2019-10-04T07:43:22.719617Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['一加', '手机', '5', '要', '做', '市面', '最', '轻薄']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba.lcut('一加手机5要做市面最轻薄')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:43:41.866387Z",
     "start_time": "2019-10-04T07:43:33.519719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 前一个和后一个拼接起来 形成2gram\n",
    "_2_gram_words = [TOKENS[i] +TOKENS[i+1] for i in range(len(TOKENS)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:43:52.576727Z",
     "start_time": "2019-10-04T07:43:52.491954Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['此外自', '自本周', '本周6', '6月', '月12', '12日起', '日起除', '除小米', '小米手机', '手机6']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_2_gram_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:44:14.617762Z",
     "start_time": "2019-10-04T07:44:07.210569Z"
    }
   },
   "outputs": [],
   "source": [
    "# 统计2 gram 词频\n",
    "_2_gram_words_count = Counter(_2_gram_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:44:20.232726Z",
     "start_time": "2019-10-04T07:44:20.085120Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('流年', 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_count.most_common()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:45:36.423855Z",
     "start_time": "2019-10-04T07:45:36.415873Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_1_gram_count(word):\n",
    "    '''获取1_gram的词频 \n",
    "    如果Word存在词典中返回词频\n",
    "    否则 返回 词频最小的那个数 '''\n",
    "    if word not in words_count:\n",
    "        return words_count.most_common()[-1][-1]\n",
    "    else:\n",
    "        return words_count[word]\n",
    "    \n",
    "def get_2_gram_count(word):\n",
    "    '''获取2_gram的词频\n",
    "        如果Word存在词典中返回词频\n",
    "    否则 返回 词频最小的那个数 '''\n",
    "    \n",
    "    if word not in _2_gram_words_count:\n",
    "        return _2_gram_words_count.most_common()[-1][-1]\n",
    "    else:\n",
    "        return _2_gram_words_count[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:46:23.560724Z",
     "start_time": "2019-10-04T07:46:23.550751Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_gram_count(word, wc):\n",
    "    ''' 从传入的 词典中查找 给定词的词频\n",
    "        word:需要查询的词\n",
    "        wc: word count\n",
    "        融合了N_gram的方法 \n",
    "    '''\n",
    "    if word in wc:\n",
    "        return wc[word]\n",
    "    else:\n",
    "        return wc.most_common()[-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:46:29.506816Z",
     "start_time": "2019-10-04T07:46:29.496855Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gram_count('xxx', words_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:46:38.379074Z",
     "start_time": "2019-10-04T07:46:35.355166Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gram_count('xxx', _2_gram_words_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:48:24.038353Z",
     "start_time": "2019-10-04T07:48:24.023392Z"
    }
   },
   "outputs": [],
   "source": [
    "def two_gram_model(sentence):\n",
    "    ''' 获取给定句子 2gram 的概率\n",
    "    sentence: 需要计算概率的句子\n",
    "    '''\n",
    "    # 切词\n",
    "    token = jieba.lcut(sentence)\n",
    "    \n",
    "    probability = 1 \n",
    "    \n",
    "    for i in range(len(token)-1 ):\n",
    "        word = token[i]\n",
    "        \n",
    "        next_word = token[i+1]\n",
    "        \n",
    "        _two_gram_c = get_gram_count(word+next_word, _2_gram_words_count)\n",
    "        \n",
    "        _one_gram_c = get_gram_count(next_word, words_count)# 为什么是 next_word？？\n",
    "        \n",
    "        pro = _two_gram_c / _one_gram_c  # 不应该是 _2_gram_words_count 词典大小吗？？\n",
    "        \n",
    "        probability *= pro \n",
    "        \n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:48:29.713167Z",
     "start_time": "2019-10-04T07:48:29.621413Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.741787766101897e-37"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_gram_model('此外自本周6月12日起除小米手机6等15款机型')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:48:38.280243Z",
     "start_time": "2019-10-04T07:48:35.010990Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.239387998125227e-10"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_gram_model('前天早上吃晚饭的时候')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:48:46.786481Z",
     "start_time": "2019-10-04T07:48:43.592031Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3509374521948863e-13"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_gram_model('前天早上吃早饭的时候')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不科学呀？？ 是语料库太小吗？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:48:54.222586Z",
     "start_time": "2019-10-04T07:48:52.517147Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.497849335963889e-07"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_gram_model('我请你吃火锅')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:49:02.916322Z",
     "start_time": "2019-10-04T07:48:59.653054Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.872460383977198e-10"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_gram_model('我请你吃日料大餐')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:49:08.375713Z",
     "start_time": "2019-10-04T07:49:08.352784Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.364629466681511e-07"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_gram_model('这个人来自清华大学')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:49:17.171213Z",
     "start_time": "2019-10-04T07:49:13.861038Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.607121764623602e-10"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_gram_model('这个人来自秦华大学')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:49:25.736263Z",
     "start_time": "2019-10-04T07:49:22.536821Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3702130336795815e-09"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_gram_model('这个花特别好看')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:49:32.833304Z",
     "start_time": "2019-10-04T07:49:31.225573Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.397130259238779e-11"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_gram_model('花这特别好看')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:50:13.683991Z",
     "start_time": "2019-10-04T07:49:38.449244Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: 一个好看的蓝色的篮球看着一个好看的小猫 with prob: 2.0574830941906373e-27\n",
      "sentence: 一个好看的蓝色的小猫坐在这个小小的好看的桌子 with prob: 1.347855598972603e-27\n",
      "sentence: 一个蓝色的桌子坐在这个好看的篮球 with prob: 8.634099335303037e-24\n",
      "sentence: 这个好看的桌子坐在这个蓝色的篮球 with prob: 8.634099335303037e-24\n",
      "sentence: 这个蓝色的小猫看见一个蓝色的篮球 with prob: 1.1785864248617108e-22\n",
      "sentence: 一个好看的桌子看着这个小小的蓝色的小小的好看的好看的好看的篮球 with prob: 9.828123416912974e-45\n",
      "sentence: 这个好看的蓝色的小猫坐在一个蓝色的小小的篮球 with prob: 2.1163832873437077e-31\n",
      "sentence: 这个好看的小小的小猫听着这个蓝色的蓝色的好看的蓝色的蓝色的桌子 with prob: 1.0466846666432414e-47\n",
      "sentence: 这个蓝色的小小的小猫看着这个小小的桌子 with prob: 3.633204988117069e-20\n",
      "sentence: 这个好看的蓝色的蓝色的蓝色的小猫听着一个蓝色的篮球 with prob: 1.251575610479878e-38\n"
     ]
    }
   ],
   "source": [
    "for sen in [get_generation_by_gram(simple_grammar,'sentence', stmt_split='=>', or_split='|') for _ in range(10)]:\n",
    "    print('sentence: {} with prob: {}'.format(sen, two_gram_model(sen)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:50:43.322657Z",
     "start_time": "2019-10-04T07:50:19.200202Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "明天晚上请你吃大餐，我们一起吃苹果 is more possible\n",
      "---- 今天晚上请你吃大餐，我们一起吃日料 with probility 1.251575610479878e-38\n",
      "---- 明天晚上请你吃大餐，我们一起吃苹果 with probility 1.251575610479878e-38\n",
      "真是一只好看的小猫 is more possible\n",
      "---- 真事一只好看的小猫 with probility 1.251575610479878e-38\n",
      "---- 真是一只好看的小猫 with probility 1.251575610479878e-38\n",
      "今晚火锅去吃我 is more possible\n",
      "---- 今晚我去吃火锅 with probility 1.251575610479878e-38\n",
      "---- 今晚火锅去吃我 with probility 1.251575610479878e-38\n",
      "养乐多绿来一杯 is more possible\n",
      "---- 洋葱奶昔来一杯 with probility 1.251575610479878e-38\n",
      "---- 养乐多绿来一杯 with probility 1.251575610479878e-38\n"
     ]
    }
   ],
   "source": [
    "sentences_compared = [\n",
    "    \"今天晚上请你吃大餐，我们一起吃日料 明天晚上请你吃大餐，我们一起吃苹果\",\n",
    "    \"真事一只好看的小猫 真是一只好看的小猫\",\n",
    "    \"今晚我去吃火锅 今晚火锅去吃我\",\n",
    "    \"洋葱奶昔来一杯 养乐多绿来一杯\"\n",
    "]\n",
    "\n",
    "for s in sentences_compared:\n",
    "    s1, s2 = s.split()\n",
    "    p1, p2 =  two_gram_model(sen),two_gram_model(sen)\n",
    "    \n",
    "    better = s1 if p1 > p2 else s2\n",
    "    \n",
    "    print('{} is more possible'.format(better))\n",
    "    print('-'*4 + ' {} with probility {}'.format(s1, p1))\n",
    "    print('-'*4 + ' {} with probility {}'.format(s2, p2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感觉结果和想象中的不一样呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义你自己的语法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:52:54.057836Z",
     "start_time": "2019-10-04T07:52:54.046864Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'韩信很善于自以为是'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Self_gram = \"\"\"\n",
    "person_info = 姓名 程度副词* 擅长 特点\n",
    "姓名 = 刘邦 | 韩信| 项羽| 秦始皇\n",
    "程度副词* =  程度副词|  程度副词*\n",
    "程度副词 = 很 | 非常 | 特别 | 极其| 总是\n",
    "擅长 = 喜欢 | 善于 | 擅长| 热衷\n",
    "特点 = 统兵打仗 | 统帅 | 打仗 | 屠杀功臣| 自以为是| 分封诸侯| 泰山封禅| 周游天下| 灭国大战\n",
    "\"\"\"\n",
    "get_generation_by_gram(Self_gram, 'person_info', stmt_split='=', or_split='|')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:53:01.961687Z",
     "start_time": "2019-10-04T07:53:01.950721Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_generation_n_by_gram(grammar, target, n):\n",
    "    \"\"\"根据语法树产生n条语句\"\"\"\n",
    "    result = []\n",
    "    for i in range(n):\n",
    "        sentence = get_generation_by_gram(grammar, target=target, stmt_split='=', or_split='|') \n",
    "        result.append(sentence)\n",
    "        \n",
    "    return result\n",
    "sentences_n = get_generation_n_by_gram(Self_gram, 'person_info', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:53:08.938020Z",
     "start_time": "2019-10-04T07:53:08.932035Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['刘邦总是喜欢自以为是', '项羽总是热衷统帅', '刘邦总是喜欢屠杀功臣', '韩信总是善于周游天下', '刘邦很善于打仗']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 获得最优质的的语言"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:53:44.055054Z",
     "start_time": "2019-10-04T07:53:28.401970Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best: ('刘邦很善于屠杀功臣', 1.251575610479878e-38) \n",
      "\n",
      "---- 刘邦很善于屠杀功臣 with probility 1.251575610479878e-38\n",
      "---- 秦始皇特别善于屠杀功臣 with probility 1.251575610479878e-38\n",
      "---- 秦始皇很热衷泰山封禅 with probility 1.251575610479878e-38\n",
      "---- 刘邦特别喜欢屠杀功臣 with probility 1.251575610479878e-38\n",
      "---- 刘邦极其善于自以为是 with probility 1.251575610479878e-38\n"
     ]
    }
   ],
   "source": [
    "def generate_best(grammar, target, n):\n",
    "    '''该函数输入一个语法 + 语言模型，能够生成n个句子，并能选择一个最合理的句子\n",
    "    '''\n",
    "    sentences = []\n",
    "    for i in range(n):\n",
    "        sentence = get_generation_by_gram(grammar, target=target, stmt_split='=', or_split='|') \n",
    "        sentences.append(sentence)\n",
    "        \n",
    "    result = [(sentence, two_gram_model(sen)) for sentence in sentences]\n",
    "    \n",
    "    sorted(result, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return result[0],result\n",
    "best,all_result = generate_best(Self_gram, 'person_info', 5)\n",
    "print(\"best:\",best,'\\n')\n",
    "for item in all_result:\n",
    "    print('-'*4 + ' {} with probility {}'.format(item[0], item[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 使用新数据源完成语言模型的训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:54:06.815153Z",
     "start_time": "2019-10-04T07:54:05.292228Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3020: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "copurs_data = pd.read_csv(\"./movie_comments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:54:12.628631Z",
     "start_time": "2019-10-04T07:54:12.622615Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261497, 5)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copurs_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:54:12.660511Z",
     "start_time": "2019-10-04T07:54:12.631587Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id         object\n",
       "link       object\n",
       "name       object\n",
       "comment    object\n",
       "star       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copurs_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:54:23.470585Z",
     "start_time": "2019-10-04T07:54:23.371850Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京意淫到了脑残的地步，看了恶心想吐</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>中二得很</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                        link name  \\\n",
       "0  1  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "1  2  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "2  3  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "3  4  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "4  5  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "\n",
       "                                             comment star  \n",
       "0                                 吴京意淫到了脑残的地步，看了恶心想吐    1  \n",
       "1  首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...    2  \n",
       "2  吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...    2  \n",
       "3                      凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。    4  \n",
       "4                                               中二得很    1  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copurs_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:54:31.258748Z",
     "start_time": "2019-10-04T07:54:31.221846Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2760"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copurs_data.name.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:54:41.518293Z",
     "start_time": "2019-10-04T07:54:41.469425Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_preprocessinng(content):\n",
    "    '''去除文本中的非汉字字符'''\n",
    "    return ''.join(re.findall('\\w+',content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:54:47.795533Z",
     "start_time": "2019-10-04T07:54:46.816156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 972 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 文本清洗 去除非汉字字符\n",
    "copurs_data['comment'] = copurs_data['comment'].apply(lambda x: data_preprocessinng(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:54:53.225967Z",
     "start_time": "2019-10-04T07:54:53.213001Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京意淫到了脑残的地步看了恶心想吐</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>首映礼看的太恐怖了这个电影不讲道理的完全就是吴京在实现他这个小粉红的英雄梦各种装备轮番上场视...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京的炒作水平不输冯小刚但小刚至少不会用主旋律来炒作吴京让人看了不舒服为了主旋律而主旋律为了...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>凭良心说好看到不像战狼1的续集完虐湄公河行动</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>中二得很</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                        link name  \\\n",
       "0  1  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "1  2  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "2  3  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "3  4  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "4  5  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "\n",
       "                                             comment star  \n",
       "0                                  吴京意淫到了脑残的地步看了恶心想吐    1  \n",
       "1  首映礼看的太恐怖了这个电影不讲道理的完全就是吴京在实现他这个小粉红的英雄梦各种装备轮番上场视...    2  \n",
       "2  吴京的炒作水平不输冯小刚但小刚至少不会用主旋律来炒作吴京让人看了不舒服为了主旋律而主旋律为了...    2  \n",
       "3                             凭良心说好看到不像战狼1的续集完虐湄公河行动    4  \n",
       "4                                               中二得很    1  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copurs_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:54:58.788083Z",
     "start_time": "2019-10-04T07:54:58.731240Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 49.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 提取所有comments\n",
    "all_comment = ''.join(copurs_data.comment.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:57:35.952543Z",
     "start_time": "2019-10-04T07:55:04.482845Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 切词\n",
    "all_token_list = jieba.lcut(all_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:57:53.793812Z",
     "start_time": "2019-10-04T07:57:47.525608Z"
    }
   },
   "outputs": [],
   "source": [
    "# 生成2gram语料\n",
    "_2_gram_words_new = [all_token_list[i] +all_token_list[i+1] for i in range(len(all_token_list)-1)]\n",
    "_2_gram_words_count = Counter(_2_gram_words_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:58:04.864184Z",
     "start_time": "2019-10-04T07:58:03.750162Z"
    }
   },
   "outputs": [],
   "source": [
    "_1_gram_words_count = Counter(all_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:58:15.976447Z",
     "start_time": "2019-10-04T07:58:15.957500Z"
    }
   },
   "outputs": [],
   "source": [
    "def prob_1(word,words_freq_counter):\n",
    "    '''1gram'''\n",
    "    return _1_gram_words_count[word] / sum(words_freq_counter.values()) if word in _1_gram_words_count else 1/sum(words_freq_counter.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:58:24.763934Z",
     "start_time": "2019-10-04T07:58:24.732019Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001309328166445808"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_1('我们',_1_gram_words_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:58:32.879220Z",
     "start_time": "2019-10-04T07:58:32.868250Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['吴京', '意淫', '到', '了', '脑残', '的', '地步', '看', '了', '恶心']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_token_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:58:41.185994Z",
     "start_time": "2019-10-04T07:58:41.177018Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.685931062701999e-06"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_1('吹牛',_1_gram_words_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:59:45.708378Z",
     "start_time": "2019-10-04T07:59:37.294888Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best: ('项羽特别热衷打仗', 6.159629219289829e-35) \n",
      "\n",
      "---- 项羽特别热衷打仗 with probility 6.159629219289829e-35\n",
      "---- 刘邦非常善于周游天下 with probility 6.159629219289829e-35\n",
      "---- 秦始皇总是热衷统兵打仗 with probility 6.159629219289829e-35\n",
      "---- 韩信极其喜欢屠杀功臣 with probility 6.159629219289829e-35\n",
      "---- 韩信极其热衷统帅 with probility 6.159629219289829e-35\n"
     ]
    }
   ],
   "source": [
    "# 改进2gram函数：使得它对所有新的 2 gram语料库兼容 General purpose\n",
    "def two_gram_model_general(sentence,_1_gram_words_count,  _2_gram_words_count):\n",
    "    ''' 获取二元词的概率\n",
    "    sentence:\n",
    "    '''\n",
    "    # 切词\n",
    "    token = jieba.lcut(sentence)\n",
    "    \n",
    "    probability = 1 \n",
    "    \n",
    "    for i in range(len(token)-1 ):\n",
    "        word = token[i]\n",
    "        \n",
    "        next_word = token[i+1]\n",
    "        \n",
    "        _two_gram_c = get_gram_count(word+next_word, _2_gram_words_count)\n",
    "        \n",
    "        _one_gram_c = get_gram_count(next_word, _1_gram_words_count)# 为什么是 next_word？？\n",
    "        \n",
    "        pro = _two_gram_c / _one_gram_c  # 不应该是 _2_gram_words_count 词典大小吗？？\n",
    "        \n",
    "        probability *= pro \n",
    "        \n",
    "    return probability\n",
    "\n",
    "def generate_best_general(grammar, target, n):\n",
    "    '''该函数输入一个语法 + 语言模型，能够生成n个句子，并能选择一个最合理的句子\n",
    "    '''\n",
    "    sentences = []\n",
    "    for i in range(n):\n",
    "        sentence = get_generation_by_gram(grammar, target=target, stmt_split='=', or_split='|') \n",
    "        sentences.append(sentence)\n",
    "        \n",
    "    result = [(sentence, two_gram_model_general(sen,_1_gram_words_count ,_2_gram_words_count)) for sentence in sentences]\n",
    "    \n",
    "    sorted(result, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return result[0],result\n",
    "\n",
    "best,all_result = generate_best_general(Self_gram, 'person_info', 5)\n",
    "print(\"best:\",best,'\\n')\n",
    "for item in all_result:\n",
    "    print('-'*4 + ' {} with probility {}'.format(item[0], item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T08:00:13.368363Z",
     "start_time": "2019-10-04T08:00:06.767992Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best: ('高晓松很非常null善于说相声', 6.159629219289829e-35) \n",
      "\n",
      "---- 高晓松很非常null善于说相声 with probility 6.159629219289829e-35\n",
      "---- 岳云鹏null喜欢拍电影 with probility 6.159629219289829e-35\n",
      "---- 岳云鹏null善于损人 with probility 6.159629219289829e-35\n",
      "---- 岳云鹏特别null喜欢说相声 with probility 6.159629219289829e-35\n",
      "---- 岳云鹏null善于作词 with probility 6.159629219289829e-35\n"
     ]
    }
   ],
   "source": [
    "new_gram = \"\"\"\n",
    "person_info = 姓名 程度副词* 擅长 特点\n",
    "姓名 =高晓松 | 岳云鹏| 于谦\n",
    "程度副词* = null | 程度副词 程度副词*\n",
    "程度副词 = 很 | 非常 | 特别 | 极其\n",
    "擅长 = 喜欢 | 善于 | 擅长\n",
    "特点 = 作词 | 作曲 | 喝酒 | 损人| 说相声| 拍电影\n",
    "\"\"\"\n",
    "best,all_result = generate_best_general(new_gram, 'person_info', 5)\n",
    "print(\"best:\",best,'\\n')\n",
    "for item in all_result:\n",
    "    print('-'*4 + ' {} with probility {}'.format(item[0], item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T08:02:25.260419Z",
     "start_time": "2019-10-04T08:02:24.428643Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这个电影真好看 is more possible\n",
      "---- 这个电影真好看 with probility 5.232056201773159e-06\n",
      "---- 这个电影正好看 with probility 6.359638906607634e-09\n",
      "晚上一起去看电影啊 is more possible\n",
      "---- 晚上一起去看电影啊 with probility 2.376825382615988e-11\n",
      "---- 一起去看晚上的电影啊 with probility 6.263654551953375e-13\n",
      "这个电影还行吧 is more possible\n",
      "---- 这个电影还行吧 with probility 5.177673459742868e-06\n",
      "---- 这个电影也就一般般吧 with probility 2.744540398580013e-09\n",
      "太狗血了这个电影 is more possible\n",
      "---- 太狗血了这个电影 with probility 3.4263808433000927e-08\n",
      "---- 这个电影拍的跟屎一样 with probility 8.15996101542436e-12\n"
     ]
    }
   ],
   "source": [
    "need_compared = [\n",
    "    '这个电影真好看 这个电影正好看',\n",
    "    '晚上一起去看电影啊 一起去看晚上的电影啊',\n",
    "    '这个电影还行吧 这个电影也就一般般吧',\n",
    "    '太狗血了这个电影 这个电影拍的跟屎一样'\n",
    "]\n",
    "\n",
    "for s in need_compared:\n",
    "    s1, s2 = s.split()\n",
    "    p1, p2 = two_gram_model_general(s1,_1_gram_words_count,  _2_gram_words_count),two_gram_model_general(s2,_1_gram_words_count,  _2_gram_words_count)\n",
    "\n",
    "    better = s1 if p1 > p2 else s2\n",
    "    \n",
    "    print('{} is more possible'.format(better))\n",
    "    print('-'*4 + ' {} with probility {}'.format(s1, p1))\n",
    "    print('-'*4 + ' {} with probility {}'.format(s2, p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T07:08:15.480421Z",
     "start_time": "2019-07-03T07:08:15.133348Z"
    }
   },
   "source": [
    "generate_best(self_definition_grammer, tokens_2gram_counter, 'self_info', 5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 这个模型有什么问题？ 你准备如何提升？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 1.语言模型用到的预料是影评领域的，所以用来预测评价电影的句子合理性比较合适，不能泛化到其他的领域.\n",
    "如果要泛化到其他领域，用来计算的词频语料要覆盖跟多领域的语料"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) 完成基于Pattern Match的语句问答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T08:05:26.896429Z",
     "start_time": "2019-10-04T08:05:26.880439Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_variable(pat):\n",
    "    '''判断 pat s否满足固定的模式\n",
    "    规则：pat 是？开头 并且除？之外都市字母\n",
    "    '''\n",
    "    return pat.startswith('?') and all(s.isalpha() for s in pat[1:])\n",
    "\n",
    "def pat_match(pattern, saying):\n",
    "    '''从头开始逐个比较pattern,和saying 检查 saying是否满足pattern bin的模式\n",
    "    '''\n",
    "    if is_variable(pattern[0]):\n",
    "        return True\n",
    "    else:\n",
    "        if pattern[0] != saying[0]:\n",
    "            return False\n",
    "        else:\n",
    "            return pat_match(pattern[1:], saying[1:])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T08:05:34.734428Z",
     "start_time": "2019-10-04T08:05:34.727440Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pat_match('I want ?X'.split(), \"I want holiday\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T08:06:02.468213Z",
     "start_time": "2019-10-04T08:06:02.457246Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pat_match('I have dreamed a ?X'.split(), \"I dreamed about dog\".split()) #false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T08:06:10.181576Z",
     "start_time": "2019-10-04T08:06:10.169643Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pat_match('I dreamed about ?X'.split(), \"I dreamed about dog\".split())#True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 获得匹配的变量:以上的函数能够判断两个 pattern 是不是相符，但是我们更加希望的是获得每个variable对应的是什么值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T08:06:18.219069Z",
     "start_time": "2019-10-04T08:06:18.214082Z"
    }
   },
   "outputs": [],
   "source": [
    "def pat_match(pattern, saying):\n",
    "    '''匹配的同时 保存模式对应的值'''\n",
    "    if is_variable(pattern[0]):\n",
    "        return pattern[0],saying[0] # 返回 pattern和对应的值，这一概念以来就可以同时匹配多个模式\n",
    "    else:\n",
    "        if pattern[0] != saying[0]:\n",
    "            return False\n",
    "        else:\n",
    "            return pat_match(pattern[1:], saying[1:])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T08:06:25.972333Z",
     "start_time": "2019-10-04T08:06:25.967335Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('?X', 'holiday')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = 'I want ?X'.split()\n",
    "saying = \"I want holiday\".split()\n",
    "pat_match(pattern, saying)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T08:06:33.811350Z",
     "start_time": "2019-10-04T08:06:33.804362Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('?X', '2+2')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pat_match(\"?X equals ?X\".split(), \"2+2 equals 2+2\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T08:06:43.721825Z",
     "start_time": "2019-10-04T08:06:43.713848Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('?X', '2+2')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 但是，如果我们的 Pattern 中具备两个变量，那么以上程序就不能解决了，我们可以对程序做如下修改:\n",
    "pat_match(\"?X equals ?Y\".split(), \"2+2 equals 2*2\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T08:07:15.064961Z",
     "start_time": "2019-10-04T08:07:15.050997Z"
    }
   },
   "outputs": [],
   "source": [
    "def pat_match(pattern, saying):\n",
    "    '''当有多个模式的时候，前面匹配上了，后面会接着匹配，直到完成为止'''\n",
    "    if not pattern or not saying:\n",
    "        return []\n",
    "    if is_variable(pattern[0]):\n",
    "        return [(pattern[0], saying[0])] + pat_match(pattern[1:], saying[1:])\n",
    "    else:\n",
    "        return pat_match(pattern[1:], saying[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T08:07:22.782308Z",
     "start_time": "2019-10-04T08:07:22.768347Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('?X', '2+2'), ('?Y', '2*2')]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pat_match(\"?X equals ?Y\".split(), \"2+2 equals 2*2\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们知道了每个变量对应的是什么，那么我们就可以很方便的使用我们定义好的模板进行替换：\n",
    "\n",
    "为了方便接下来的替换工作，我们新建立两个函数，一个是把我们解析出来的结果变成一个 dictionary，一个是依据这个 dictionary 依照我们的定义的方式进行替换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T08:07:34.488984Z",
     "start_time": "2019-10-04T08:07:34.479009Z"
    }
   },
   "outputs": [],
   "source": [
    "def pat_to_dict(patterns):\n",
    "    '''把模式转换成字段方便查找和替换\n",
    "    v2: r如果是单模式匹配多个单词 就拼接 '''\n",
    "    return {k: ' '.join(v) if isinstance(v, list) else v for k, v in patterns}\n",
    "#     return {k:v for k,v in patterns}\n",
    "\n",
    "def subsitite(rule, parsed_rules):\n",
    "    '''把句子中出现的模式映射回该模式对用的值'''\n",
    "    if not rule:\n",
    "        return []\n",
    "    return [parsed_rules.get(rule[0], rule[0])] + subsitite(rule[1:], parsed_rules)# 巧妙 递归的运用：如果当前词不再映射表中说明当前词不是模式，返回本身"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T08:07:42.211321Z",
     "start_time": "2019-10-04T08:07:42.199390Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('?X', 'iPhone')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['What', 'if', 'you', 'mean', 'if', 'you', 'got', 'a', 'iPhone']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "got_patterns = pat_match(\"I want ?X\".split(), \"I want iPhone\".split())\n",
    "print(got_patterns)\n",
    "subsitite(\"What if you mean if you got a ?X\".split(), pat_to_dict(got_patterns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T08:07:50.033389Z",
     "start_time": "2019-10-04T08:07:50.027405Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What if you mean if you got a iPhone'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(subsitite(\"What if you mean if you got a ?X\".split(), pat_to_dict(got_patterns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T08:07:57.745754Z",
     "start_time": "2019-10-04T08:07:57.735780Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What if you mean if you got a iPhone\n"
     ]
    }
   ],
   "source": [
    "def sentence_subsitite(sentence, parsed_rules):\n",
    "    '''把句子中出现的模式映射回该模式对用的值'''\n",
    "    return ' '.join(subsitite(sentence.split(), pat_to_dict(parsed_rules)))\n",
    "\n",
    "print(sentence_subsitite(\"What if you mean if you got a ?X\", got_patterns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T08:08:05.302533Z",
     "start_time": "2019-10-04T08:08:05.295552Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why does John need vacation ?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(sentence_subsitite(\"Why does ?P need ?X ?\", \n",
    "                         pat_match('?P needs ?X'.split(), \"John needs vacation\".split())\n",
    "                        )\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T08:08:12.926135Z",
     "start_time": "2019-10-04T08:08:12.919152Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('?P', 'John'), ('?X', 'vacation')]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pat_match('?P needs ?X'.split(), \"John needs vacation\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T08:09:23.242980Z",
     "start_time": "2019-10-04T08:09:23.232044Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do you need iPhone ?\n",
      "How do you think about your mother ?\n"
     ]
    }
   ],
   "source": [
    "def pat_match(pattern, saying):\n",
    "    '''当有多个模式的时候，前面匹配上了，后面会接着匹配，知道完成为止'''\n",
    "\n",
    "    if not pattern or not saying:\n",
    "        return []\n",
    "    if is_variable(pattern[0]):\n",
    "        return [(pattern[0], saying[0])] + pat_match(pattern[1:], saying[1:])\n",
    "    else:\n",
    "        return pat_match(pattern[1:], saying[1:]) if pattern[0]== saying[0] else []\n",
    "    \n",
    "# 那么如果我们现在定义一些patterns，就可以实现基于模板的对话生成了\n",
    "defined_patterns = {\n",
    "    \"I need ?X\": [\"Image you will get ?X soon\", \"Why do you need ?X ?\"], \n",
    "    \"My ?X told me something\": [\"Talk about more about your ?X\", \"How do you think about your ?X ?\"]\n",
    "}\n",
    "\n",
    "def get_response(saying, rules):\n",
    "\n",
    "    for pattern in rules:\n",
    "#         print(\"pattern:\",pattern,\"saying:\",saying)\n",
    "        match_result = pat_match(pattern.split(),saying.split())\n",
    "#         print(\"match_result: \",match_result)\n",
    "        if match_result == []:\n",
    "            continue \n",
    "        else:\n",
    "            return sentence_subsitite(random.choice(rules[pattern]), \n",
    "                                      match_result\n",
    "                                     )\n",
    "        \n",
    "print(get_response('I need iPhone', defined_patterns))\n",
    "print(get_response(\"My mother told me something\", defined_patterns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segment Match\n",
    "我们上边的这种形式，能够进行一些初级的对话了，但是我们的模式逐字逐句匹配的， \"I need iPhone\" 和 \"I need ?X\" 可以匹配，但是\"I need an iPhone\" 和 \"I need ?X\" 就不匹配了，那怎么办？\n",
    "\n",
    "为了解决这个问题，我们可以新建一个变量类型 \"?\\*X\", 这种类型多了一个星号(\\*),表示匹配多个\n",
    "\n",
    "首先，和前文类似，我们需要定义一个判断是不是匹配多个的variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T08:09:33.916420Z",
     "start_time": "2019-10-04T08:09:33.909441Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def is_pattern_segment(pattern):\n",
    "    '''一个模式匹配多个单词'''\n",
    "    return pattern.startswith('?*') and all(a.isalpha() for a in pattern[2:])\n",
    "print(is_pattern_segment('?*p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T08:09:57.218071Z",
     "start_time": "2019-10-04T08:09:57.203111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('?X', ['I', 'was', 'real', 'want', 'a', 'iphone']), True, False]\n"
     ]
    }
   ],
   "source": [
    "fail = [True,False]\n",
    "\n",
    "def is_match(rest, saying):\n",
    "    ''''''\n",
    "    if not rest and not saying:\n",
    "        # 如果带比较的2个串度到达了最后 说明匹配上了\n",
    "        return True\n",
    "    if not all(a.isalpha() for a in rest[0]):\n",
    "        return True\n",
    "    if rest[0] != saying[0]:\n",
    "        return False\n",
    "    return is_match(rest[1:], saying[1:])\n",
    "\n",
    "def segment_match(pattern, saying):\n",
    "    '''尽最大可能的匹配一个模式'''\n",
    "    seg_pat, rest = pattern[0], pattern[1:]\n",
    "    seg_pat = seg_pat.replace('?*', '?')\n",
    "    \n",
    "    if not rest:\n",
    "        return (seg_pat, saying),len(saying)\n",
    "    \n",
    "    for i, token in enumerate(saying):\n",
    "        if rest[0] == token and is_match(rest[1:], saying[(1+i):]):\n",
    "            return (seg_pat, saying[:i]), i\n",
    "\n",
    "    return (seg_pat, saying), len(saying)\n",
    "\n",
    "def pat_match_with_seg(pattern, saying):\n",
    "    ''' 一个模式匹配多个单词的情况\n",
    "    pattern:\n",
    "    saying:\n",
    "    '''\n",
    "#     print(pattern,saying)\n",
    "    if not pattern and not saying:\n",
    "        return []\n",
    "    elif not saying and pattern:  # 不能 让 \"I was real want a iphone\" 和?*X hello ?*Y\": 的匹配 一个模式匹配了所有的saying 这是不允许的\n",
    "        return fail  \n",
    "    \n",
    "    pat = pattern[0]\n",
    "    \n",
    "    if is_variable(pat):\n",
    "        return [(pat, saying[0])] + pat_match_with_seg(pattern[1:], saying[1:])\n",
    "    \n",
    "    elif is_pattern_segment(pat):\n",
    "        match,index = segment_match(pattern, saying)\n",
    "        return [match] + pat_match_with_seg(pattern[1:], saying[index:])\n",
    "    elif pat == saying[0]:\n",
    "        return pat_match_with_seg(pattern[1:], saying[1:])\n",
    "    else:\n",
    "        return fail\n",
    "    \n",
    "\n",
    "# segment_match('?*P is very good'.split(), \"My dog and my cat is very good\".split()) # (?*P, My dog and my cat) ,5   \n",
    "# print(pat_match_with_seg(\"?*X hello ?*Y\".split(), 'peter hello I need iPhone'.split()))  # ?*X hello ?*Y\n",
    "print(pat_match_with_seg(\"?*X hello ?*Y\".split(), 'I was real want a iphone'.split()))  # ?*X hello ?*Y\n",
    "# print(get_response(\"I was real want a iphone\", rules))  # I was ?*X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T08:10:05.436115Z",
     "start_time": "2019-10-04T08:10:05.428102Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('?P', ['My', 'dog']), ('?X', ['my', 'cat', 'is', 'very', 'cute'])]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pat_match_with_seg('?*P is very good and ?*X'.split(), \"My dog is very good and my cat is very cute\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T08:10:13.441658Z",
     "start_time": "2019-10-04T08:10:13.433685Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('?X', ['an', 'iPhone'])]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_pair = {\n",
    "    'I need ?X': [ \"Why do you neeed ?X\" ],\n",
    "    \"I dont like my ?X\": [\"What bad things did ?X do for you?\"]\n",
    "}\n",
    "\n",
    "pat_match_with_seg('I need ?*X'.split(), \n",
    "                  \"I need an iPhone\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T08:10:26.177582Z",
     "start_time": "2019-10-04T08:10:26.172595Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why do you need an iPhone'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_subsitite(\"Why do you  need ?X\", \n",
    "                   pat_match_with_seg('I need ?*X'.split(),\"I need an iPhone\".split())\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 现在是你的时间了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T08:10:57.286340Z",
     "start_time": "2019-10-04T08:10:57.282385Z"
    }
   },
   "outputs": [],
   "source": [
    "#我们给大家一些例子: \n",
    "    \n",
    "rules = {\n",
    "    \"?*X hello ?*Y\": [\"Hi, how do you do?\"],\n",
    "    \"I was ?*X\": [\"Were you really ?X ?\", \"I already knew you were ?X .\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 问题1\n",
    "编写一个程序, get_response(saying, response_rules)输入是一个字符串 + 我们定义的 rules，例如上边我们所写的 pattern， 输出是一个回答。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T08:11:33.438610Z",
     "start_time": "2019-10-04T08:11:33.367793Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, how do you do?\n",
      "I already knew you were real want a iphone .\n"
     ]
    }
   ],
   "source": [
    "rules = {\n",
    "    \"?*X hello ?*Y\": [\"Hi, how do you do?\"],\n",
    "    \"I was ?*X\": [\"Were you really ?X ?\", \"I already knew you were ?X .\"]\n",
    "}\n",
    "\n",
    "def get_response(saying, rules):\n",
    "\n",
    "    for pattern in rules:\n",
    "        match_result = pat_match_with_seg(pattern.split(),saying.split())\n",
    "        if match_result[-1]:\n",
    "            return sentence_subsitite(random.choice(rules[pattern]), \n",
    "                                      match_result\n",
    "                                     )\n",
    "        \n",
    "print(get_response('peter hello I need iPhone', rules))  # ?*X hello ?*Y\n",
    "print(get_response(\"I was real want a iphone\", rules))  # I was ?*X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 问题2\n",
    "改写以上程序，将程序变成能够支持中文输入的模式。 提示: 你可以需用用到 jieba 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T08:39:03.452516Z",
     "start_time": "2019-10-04T08:39:01.318252Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['?*x', '我', '想要', '?*y', '将', '程序', '变成', '能']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_pattern_contain_chinese(sentence):\n",
    "    '''使用递归实现\n",
    "    ASCII码为标准符号、数字、英文等进行了保留，取值范围是0～127\n",
    "    还有一部分作为扩展ASCII码128～255\n",
    "    '''\n",
    "\n",
    "    start = 0\n",
    "    \n",
    "    if len(sentence) ==  0:\n",
    "        return []\n",
    "    \n",
    "    i = 1\n",
    "    if ord(sentence[0]) <128:\n",
    "        while (i < len(sentence)) and (ord(sentence[i]) <128):\n",
    "                i +=1\n",
    "    else:\n",
    "        while (i < len(sentence)) and (ord(sentence[i]) >= 128):\n",
    "                i +=1\n",
    "    # 如果是中文就对中文进行分词\n",
    "    if ord(sentence[0]) >= 128:\n",
    "        temp = jieba.lcut(sentence[start:i])\n",
    "    else:\n",
    "        temp = [sentence[start:i]]\n",
    "\n",
    "    return temp + split_pattern_contain_chinese(sentence[i:])\n",
    "\n",
    "split_pattern_contain_chinese('?*x我想要?*y将程序变成能')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T08:41:00.036552Z",
     "start_time": "2019-10-04T08:41:00.028574Z"
    }
   },
   "outputs": [],
   "source": [
    "# 匹配中文回答\n",
    "def get_chiness_respose(saying, rules):\n",
    "    ''''''\n",
    "    for pattern in rules:\n",
    "        match_result = pat_match_with_seg(split_pattern_contain_chinese(pattern),\n",
    "                                          split_pattern_contain_chinese(saying))\n",
    "#         print(match_result)\n",
    "        if match_result[-1]:\n",
    "            return sentence_subsitite(random.choice(rules[pattern]), \n",
    "                                      match_result\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T08:41:08.291481Z",
     "start_time": "2019-10-04T08:41:08.265535Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "喜欢赵孟頫的哪里？\n",
      "你不想要学习吗？\n"
     ]
    }
   ],
   "source": [
    "def pat_to_dict(patterns):\n",
    "    '''把模式转换成字段方便查找和替换\n",
    "    v3: r如果是单模式匹配多个单词 就拼接 中文拼接不需要空格 '''\n",
    "    return {k: ''.join(v) if isinstance(v, list) else v for k, v in patterns}\n",
    "\n",
    "def sentence_subsitite(sentence, parsed_rules):\n",
    "    '''把句子中出现的模式映射回该模式对用的值\n",
    "    中文拼接不需要空格'''\n",
    "    return ''.join(subsitite(split_pattern_contain_chinese(sentence), pat_to_dict(parsed_rules)))\n",
    "\n",
    "chinese_rules = {\n",
    "    '?*x我想要?*y': ['?x想问你，你觉得?y有什么意义呢?', '为什么你想?y', '?x觉得... 你可以想想你很快就可以有?y了', '你看?x像?y不', '我看你就像?y'],\n",
    "    '?*x喜欢?*y': ['喜欢?y的哪里？', '?y有什么好的呢？', '你想和?y在一起吗？'],\n",
    "    '?*x讨厌?*y': ['?y怎么会那么讨厌呢?', '讨厌?y的哪里？', '?y有什么不好呢？', '你不想要?y吗？']\n",
    "}\n",
    "print(get_chiness_respose('我喜欢赵孟頫', chinese_rules))\n",
    "print(get_chiness_respose('我讨厌学习', chinese_rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T08:41:36.081108Z",
     "start_time": "2019-10-04T08:41:15.975912Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "路人甲: q\n",
      "路人乙:对不起，我什么也没看见，什么也没听到，我什么都不知道!!\n"
     ]
    }
   ],
   "source": [
    "def chiness_dialogue(rules):\n",
    "    '''实现中文对话'''\n",
    "    while True:\n",
    "        saying = input(\"路人甲: \")\n",
    "        if saying.lower() not in ['bye','quit','q','exit']:\n",
    "            respone=get_chiness_respose(saying, rules)\n",
    "            print(f\"路人乙:{respone}\")\n",
    "        else:\n",
    "            print(\"路人乙:对不起，我什么也没看见，什么也没听到，我什么都不知道!!\")\n",
    "            break\n",
    "chiness_dialogue(chinese_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 问题3\n",
    "多设计一些模式，让这个程序变得更好玩，多和大家交流，看看大家有什么好玩的模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T08:45:16.164216Z",
     "start_time": "2019-10-04T08:45:16.144260Z"
    }
   },
   "outputs": [],
   "source": [
    "rule_responses = {\n",
    "    '?*x hello ?*y': ['How do you do', 'Please state your problem'],\n",
    "    '?*x I want ?*y': ['what would it mean if you got ?y', 'Why do you want ?y', 'Suppose you got ?y soon'],\n",
    "    '?*x if ?*y': ['Do you really think its likely that ?y', 'Do you wish that ?y', 'What do you think about ?y', 'Really-- if ?y'],\n",
    "    '?*x no ?*y': ['why not?', 'You are being a negative', 'Are you saying \\'No\\' just to be negative?'],\n",
    "    '?*x I was ?*y': ['Were you really', 'Perhaps I already knew you were ?y', 'Why do you tell me you were ?y now?'],\n",
    "    '?*x I feel ?*y': ['Do you often feel ?y ?', 'What other feelings do you have?'],\n",
    "    '?*x你好?*y': ['你好呀', '请告诉我你的问题'],\n",
    "    '?*x我想?*y': ['你觉得?y有什么意义呢？', '为什么你想?y', '你可以想想你很快就可以?y了'],\n",
    "    '?*x我想要?*y': ['?x想问你，你觉得?y有什么意义呢?', '为什么你想?y', '?x觉得... 你可以想想你很快就可以有?y了', '你看?x像?y不', '我看你就像?y'],\n",
    "    '?*x喜欢?*y': ['喜欢?y的哪里？', '?y有什么好的呢？', '你想要?y吗？'],\n",
    "    '?*x讨厌?*y': ['?y怎么会那么讨厌呢?', '讨厌?y的哪里？', '?y有什么不好呢？', '你不想要?y吗？'],\n",
    "    '?*xAI?*y': ['你为什么要提AI的事情？', '你为什么觉得AI要解决你的问题？'],\n",
    "    '?*x机器人?*y': ['你为什么要提机器人的事情？', '你为什么觉得机器人要解决你的问题？'],\n",
    "    '?*x对不起?*y': ['不用道歉', '你为什么觉得你需要道歉呢?'],\n",
    "    '?*x我记得?*y': ['你经常会想起这个吗？', '除了?y你还会想起什么吗？', '你为什么和我提起?y'],\n",
    "    '?*x如果?*y': ['你真的觉得?y会发生吗？', '你希望?y吗?', '真的吗？如果?y的话', '关于?y你怎么想？'],\n",
    "    '?*x我?*z梦见?*y':['真的吗? --- ?y', '你在醒着的时候，以前想象过?y吗？', '你以前梦见过?y吗'],\n",
    "    '?*x妈妈?*y': ['你家里除了?y还有谁?', '嗯嗯，多说一点和你家里有关系的', '她对你影响很大吗？'],\n",
    "    '?*x爸爸?*y': ['你家里除了?y还有谁?', '嗯嗯，多说一点和你家里有关系的', '他对你影响很大吗？', '每当你想起你爸爸的时候， 你还会想起其他的吗?'],\n",
    "    '?*x我愿意?*y': ['我可以帮你?y吗？', '你可以解释一下，为什么想?y'],\n",
    "    '?*x我很难过，因为?*y': ['我听到你这么说， 也很难过', '?y不应该让你这么难过的'],\n",
    "    '?*x难过?*y': ['我听到你这么说， 也很难过',\n",
    "                 '不应该让你这么难过的，你觉得你拥有什么，就会不难过?',\n",
    "                 '你觉得事情变成什么样，你就不难过了?'],\n",
    "    '?*x就像?*y': ['你觉得?x和?y有什么相似性？', '?x和?y真的有关系吗？', '怎么说？'],\n",
    "    '?*x和?*y都?*z': ['你觉得?z有什么问题吗?', '?z会对你有什么影响呢?'],\n",
    "    '?*x和?*y一样?*z': ['你觉得?z有什么问题吗?', '?z会对你有什么影响呢?'],\n",
    "    '?*x我是?*y': ['真的吗？', '?x想告诉你，或许我早就知道你是?y', '你为什么现在才告诉我你是?y'],\n",
    "    '?*x我是?*y吗': ['如果你是?y会怎么样呢？', '你觉得你是?y吗', '如果你是?y，那一位着什么?'],\n",
    "    '?*x你是?*y吗':  ['你为什么会对我是不是?y感兴趣?', '那你希望我是?y吗', '你要是喜欢， 我就会是?y'],\n",
    "    '?*x你是?*y' : ['为什么你觉得我是?y'],\n",
    "    '?*x因为?*y' : ['?y是真正的原因吗？', '你觉得会有其他原因吗?'],\n",
    "    '?*x我不能?*y': ['你或许现在就能?*y', '如果你能?*y,会怎样呢？'],\n",
    "    '?*x我觉得?*y': ['你经常这样感觉吗？', '除了到这个，你还有什么其他的感觉吗？'],\n",
    "    '?*x我?*y你?*z': ['其实很有可能我们互相?y'],\n",
    "    '?*x你为什么不?*y': ['你自己为什么不?y', '你觉得我不会?y', '等我心情好了，我就?y'],\n",
    "    '?*x好的?*y': ['好的', '你是一个很正能量的人'],\n",
    "    '?*x嗯嗯?*y': ['好的', '你是一个很正能量的人'],\n",
    "    '?*x不嘛?*y': ['为什么不？', '你有一点负能量', '你说 不，是想表达不想的意思吗？'],\n",
    "    '?*x不要?*y': ['为什么不？', '你有一点负能量', '你说 不，是想表达不想的意思吗？'],\n",
    "    '?*x有些人?*y': ['具体是哪些人呢?'],\n",
    "    '?*x有的人?*y': ['具体是哪些人呢?'],\n",
    "    '?*x某些人?*y': ['具体是哪些人呢?'],\n",
    "    '?*x每个人?*y': ['我确定不是人人都是', '你能想到一点特殊情况吗？', '例如谁？', '你看到的其实只是一小部分人'],\n",
    "    '?*x所有人?*y': ['我确定不是人人都是', '你能想到一点特殊情况吗？', '例如谁？', '你看到的其实只是一小部分人'],\n",
    "    '?*x总是?*y': ['你能想到一些其他情况吗?', '例如什么时候?', '你具体是说哪一次？', '真的---总是吗？'],\n",
    "    '?*x一直?*y': ['你能想到一些其他情况吗?', '例如什么时候?', '你具体是说哪一次？', '真的---总是吗？'],\n",
    "    '?*x或许?*y': ['你看起来不太确定'],\n",
    "    '?*x可能?*y': ['你看起来不太确定'],\n",
    "    '?*x他们是?*y吗？': ['你觉得他们可能不是?y？'],\n",
    "    '?*x': ['很有趣', '请继续', '我不太确定我很理解你说的, 能稍微详细解释一下吗?']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T08:49:22.283541Z",
     "start_time": "2019-10-04T08:45:34.173020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "路人甲: 小米你为什么喜欢机器人？\n",
      "路人乙:喜欢机器人？的哪里？\n",
      "路人甲: 我喜欢赵孟頫\n",
      "路人乙:你想要赵孟頫吗？\n",
      "路人甲: 呵呵\n",
      "路人乙:很有趣\n",
      "路人甲: 这个我觉得不可能\n",
      "路人乙:你经常这样感觉吗？\n",
      "路人甲: 我不喜欢学习\n",
      "路人乙:学习有什么好的呢？\n",
      "路人甲: q\n",
      "路人乙:对不起，我什么也没看见，什么也没听到，我什么都不知道!!\n"
     ]
    }
   ],
   "source": [
    "def chiness_dialogue(rules):\n",
    "    '''实现中文对话'''\n",
    "    while True:\n",
    "        saying = input(\"路人甲: \")\n",
    "        if saying.lower() not in ['bye','quit','q','exit']:\n",
    "            respone=get_chiness_respose(saying, rules)\n",
    "            print(f\"路人乙:{respone}\")\n",
    "        else:\n",
    "            print(\"路人乙:对不起，我什么也没看见，什么也没听到，我什么都不知道!!\")\n",
    "            break\n",
    "chiness_dialogue(rule_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 问题4\n",
    "这样的程序有什么优点？有什么缺点？你有什么可以改进的方法吗？\n",
    "什么是数据驱动？数据驱动在这个程序里如何体现？\n",
    "数据驱动与 AI 的关系是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 优点：\n",
    "\n",
    "不需要很专业的AI知识，很容易实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 缺点：\n",
    "\n",
    "模式需要预先定义，呀穷举所有的模式也不是很现实"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 改进方法：\n",
    "\n",
    "可能需要分析很多对话日志，从中找到很多不一样的模式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 什么是数据驱动？数据驱动在这个程序里如何体现？什么是数据驱动？数据驱动在这个程序里如何体现？\n",
    "\n",
    "就如上面的 改进方法，从大量不一样的领域获取各种不同的对话语料，来驱动我们对话模型的改进"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5  数据驱动与 AI 的关系是什么？\n",
    "\n",
    "其实AI 就是以大量的数据为基础，训练有效的模型，有技术没有数据也不能产出好的产品"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
